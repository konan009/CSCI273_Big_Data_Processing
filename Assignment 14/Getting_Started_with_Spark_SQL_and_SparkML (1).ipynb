{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTgIsAbYnm4"
      },
      "source": [
        "## CS 297.2 Big Data Processing: Spark SQL and SparkML\n",
        "\n",
        "Datasets used for this notebook can be found here: https://drive.google.com/drive/folders/1qYg9SXcc9minIErchqWXR9Rq4CYj7dqf\n",
        "\n",
        "wyu@ateneo.edu\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99KJQu6LZStn"
      },
      "source": [
        "### Installing Spark on the machine\n",
        "\n",
        "Once you have installed java, the next steps should be similar. You will likely want to put the Spark application folder wherever you put your user-installed applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNbzfSSJZUNx"
      },
      "outputs": [],
      "source": [
        "!rm -r spark*\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!ls\n",
        "!tar xvf ./spark-3.5.1-bin-hadoop3.tgz > /dev/null 2>/dev/null\n",
        "!ls\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5L77xLZZaYO"
      },
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3/\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-11-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXyypJoYZeNq"
      },
      "source": [
        "____\n",
        "## Sample 1: Spark SQL examples\n",
        "\n",
        "Using SQL in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeMK4AImtBWa"
      },
      "outputs": [],
      "source": [
        "# import new dataset for plotting (for this activity is should be irdata-v3.csv.gz)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLbc3vAkZrWu"
      },
      "outputs": [],
      "source": [
        "# If Spark is installed and SPARK_HOME is set, this will find the spark installation so spark libraries can be imported.\n",
        "# findspark is necessary if you want to use Spark in the IDE of your choice.\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Imports the basic spark functions needed\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from operator import add\n",
        "import io\n",
        "\n",
        "# Sets the Spark configuration. The AppName is arbitrary, but setting the master to local\n",
        "# specifies that the application is not running on a distributed system\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkQLExample\")\n",
        "sc = SparkContext.getOrCreate(conf = conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FS2e_j8Ynm9"
      },
      "outputs": [],
      "source": [
        "# check if context if available. This might require some waiting\n",
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n40EAemYnnG"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "from pyspark import SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJm65SGCYnnN"
      },
      "outputs": [],
      "source": [
        "# create spark sql context\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhUPuTKyYnnU"
      },
      "outputs": [],
      "source": [
        "# read data from filesystem\n",
        "contraceptionData = sqlContext.read.csv('irdata-v3.csv.gz', header = True, inferSchema = True).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1jRtbeNYnnY"
      },
      "outputs": [],
      "source": [
        "# did we get the data?\n",
        "contraceptionData.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK-1jzQ_Dpkn"
      },
      "outputs": [],
      "source": [
        "contraceptionData.toPandas().info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGydnAEEavDc"
      },
      "outputs": [],
      "source": [
        "# is my data clean?\n",
        "contraceptionData.toPandas()[\"Used method in last 12 months\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxy2JjI3Ynne"
      },
      "outputs": [],
      "source": [
        "# let us start cleaning it.\n",
        "# give all null values a value of -1\n",
        "from pyspark.sql.functions import col\n",
        "contraceptionDataStringReplaced = contraceptionData.na.replace(\"\", \"-1\")\n",
        "\n",
        "# cast a string column to float\n",
        "contraceptionDataCasted = contraceptionDataStringReplaced.withColumn(\"Religion\", col(\"Religion\").cast(\"float\"))\n",
        "\n",
        "# use 2 to mean no answer since the classification model only accepts labels from 0,1,9\n",
        "notNull = contraceptionDataCasted.fillna({ 'Used method in last 12 months':2 })\n",
        "\n",
        "# Other features will be relabeled to -1\n",
        "df = notNull.fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzigN6vhdDTk"
      },
      "outputs": [],
      "source": [
        "# look at cleaned version\n",
        "df.toPandas().info()\n",
        "df.toPandas()[\"Used method in last 12 months\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HegwTsVSYnnk"
      },
      "outputs": [],
      "source": [
        "# show first few cleaned up rows\n",
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x05JRnZ5Ynnr"
      },
      "outputs": [],
      "source": [
        "# select contents of one column\n",
        "df.select(\"Land Owner\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrPPG_4dYnnw"
      },
      "outputs": [],
      "source": [
        "# filter data\n",
        "df.filter(df.Age > 30).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJJM-9viYnn3"
      },
      "outputs": [],
      "source": [
        "# groupby religion\n",
        "df.select(\"Age\",\"Residence Type\",\"Religion\").groupby(\"Religion\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_w9sMbtegUU"
      },
      "outputs": [],
      "source": [
        "# groupby method\n",
        "df.select(\"Age\",\"Residence Type\",\"Used method in last 12 months\").groupby(\"Used method in last 12 months\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdIE8s1Hfyr-"
      },
      "outputs": [],
      "source": [
        "# groupby religion\n",
        "df.select(\"Age\",\"Residence Type\",\"Religion\").filter(df.Age > 30).groupby(\"Religion\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRSlASiGYnn6"
      },
      "outputs": [],
      "source": [
        "# query as SQL\n",
        "df.registerTempTable(\"loans\")\n",
        "sqlContext.sql(\"SELECT Religion, count(*) FROM loans WHERE Age > 30 GROUP BY Religion\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWguw3m7gh62"
      },
      "source": [
        "---\n",
        "## Exercise: Studying input file details in SQL\n",
        "This activity is to process the CC CSV file using SparkQL\n",
        "\n",
        "1.   Load the file cc.csv in the SampleData directory\n",
        "2.   Create a query using both data frame and SQL mechanism to show the number of entries by gender but only count the TRUE entries with amounts greater than $1000\n",
        "3.   What do you think the cc.csv file represents?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjB0uV8YlnXN"
      },
      "outputs": [],
      "source": [
        "# show query using data frame mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WysqN3mFluMi"
      },
      "outputs": [],
      "source": [
        "# show query using SQL mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MzBbOIvYnn9"
      },
      "source": [
        "____\n",
        "## Sample 2: Lets jump to Machine Learning\n",
        "\n",
        "Using ML in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztjw_nEugHyL"
      },
      "outputs": [],
      "source": [
        "# ML stuff\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcAYFf50eFGI"
      },
      "outputs": [],
      "source": [
        "# recap on what is in the schema\n",
        "df.toPandas().info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3QvGxBHYnn_"
      },
      "outputs": [],
      "source": [
        "# split data\n",
        "train, test = df.randomSplit([0.7, 0.3], seed=30)\n",
        "train.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta9dm_oFGRB5"
      },
      "outputs": [],
      "source": [
        "test.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj-Yv0DvYnoF"
      },
      "outputs": [],
      "source": [
        "# assemble ML pipeline using LR\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['Age', 'Residence Type', 'Religion', 'Educ in single years', 'Encouraged FP', 'HH Head', 'Land Owner', 'Earns more', 'DM Contraception', 'Depression/ anxiety', 'Total CEB', 'Number of SC'],\n",
        "    outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'Used method in last 12 months', maxIter=10)\n",
        "classificationPipeline = Pipeline(stages=[assembler, lr])\n",
        "classificationPipelineModel = classificationPipeline.fit(train)\n",
        "classified = classificationPipelineModel.transform(train)\n",
        "\n",
        "# create ParamGrid for Cross Validation\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.25, 0.5, 1.0, 2.0])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "             .addGrid(lr.maxIter, [1, 5, 10, 15, 20])\n",
        "             .build())\n",
        "prediction = classificationPipelineModel.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucIvdI_PYnoJ"
      },
      "outputs": [],
      "source": [
        "# evaluate model based on default parameters and start training!\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol = \"Used method in last 12 months\")\n",
        "print('Accuracy before Cross Validation ', evaluator.evaluate(prediction))\n",
        "\n",
        "cv = CrossValidator(estimator=classificationPipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "cvModel = cv.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjyKSfFeYnoO"
      },
      "outputs": [],
      "source": [
        "# evaluate model after CV validation\n",
        "predictions_cvModel = cvModel.transform(test)\n",
        "\n",
        "print('Accuracy after Cross Validation: ', evaluator.evaluate(predictions_cvModel))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajOKXJEjmBF-"
      },
      "source": [
        "---\n",
        "## Exercise: This is so slow why not run it in the cloud\n",
        "Convert the SparkML script above into a EMR/Dataproc/HDInsight Spark job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyIGKwE-mJQn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
